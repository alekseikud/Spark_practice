{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "2530bb5307d85c10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T16:30:43.949816Z",
     "start_time": "2025-09-03T16:30:43.835948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession,Window\n",
    "from pyspark.sql.functions import col,dense_rank,when,rank"
   ],
   "id": "c6661b60b226f14b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configure connection",
   "id": "5fceb94326482547"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T16:30:50.016003Z",
     "start_time": "2025-09-03T16:30:44.992678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark=SparkSession.builder.appName(\"PagilaSpark\")\\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .config(\"spark.jars\", \"jars/postgresql-42.7.3.jar\")\\\n",
    "    .getOrCreate()\n",
    "jdbc_url = \"jdbc:postgresql://pagila:5432/postgres\"\n",
    "pg_properties = {\"user\": \"postgres\", \"password\": \"123456\", \"driver\": \"org.postgresql.Driver\"}\n"
   ],
   "id": "6fed35b7f72f619d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/03 18:30:45 WARN Utils: Your hostname, MacBook-Air-alex-5.local, resolves to a loopback address: 127.0.0.1; using 10.132.32.46 instead (on interface en0)\n",
      "25/09/03 18:30:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/09/03 18:30:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding all tables from DB",
   "id": "c15aad40623888c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T16:30:51.943639Z",
     "start_time": "2025-09-03T16:30:50.919659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "actor=spark.read.jdbc(url=jdbc_url,table=\"public.actor\",properties=pg_properties)\n",
    "address=spark.read.jdbc(url=jdbc_url,table=\"address\",properties=pg_properties)\n",
    "category=spark.read.jdbc(url=jdbc_url,table=\"category\",properties=pg_properties)\n",
    "city=spark.read.jdbc(url=jdbc_url,table=\"city\",properties=pg_properties)\n",
    "country=spark.read.jdbc(url=jdbc_url,table=\"country\",properties=pg_properties)\n",
    "customer=spark.read.jdbc(url=jdbc_url,table=\"customer\",properties=pg_properties)\n",
    "film=spark.read.jdbc(url=jdbc_url,table=\"film\",properties=pg_properties)\n",
    "film_actor=spark.read.jdbc(url=jdbc_url,table=\"film_actor\",properties=pg_properties)\n",
    "film_category=spark.read.jdbc(url=jdbc_url,table=\"film_category\",properties=pg_properties)\n",
    "inventory=spark.read.jdbc(url=jdbc_url,table=\"inventory\",properties=pg_properties)\n",
    "language=spark.read.jdbc(url=jdbc_url,table=\"language\",properties=pg_properties)\n",
    "payment=spark.read.jdbc(url=jdbc_url,table=\"payment\",properties=pg_properties)\n",
    "rental=spark.read.jdbc(url=jdbc_url,table=\"rental\",properties=pg_properties)\n"
   ],
   "id": "699e8fcc53d8463a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task implementation:",
   "id": "d67af3d40cebdd04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **1 Task:** Output the number of movies in each category, sorted in descending order.",
   "id": "62a8d249e7026fad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T16:30:57.070851Z",
     "start_time": "2025-09-03T16:30:54.877012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "category_counts=film_category.select(\"category_id\").groupBy(\"category_id\").count()\n",
    "task_1_result=category_counts.join(on=\"category_id\", how=\"inner\",other=category)\\\n",
    "                             .orderBy(col(\"count\").desc()).drop(\"last_update\")\\\n",
    "                             .withColumnRenamed(\"count\",\"number_of_films\")\\\n",
    "                             .withColumnRenamed(\"name\",\"category\")\n",
    "\n",
    "task_1_result.show()"
   ],
   "id": "9a37a9b1cf8f05f6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/03 18:30:56 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.19.0.6 executor 1): org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:346)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:298)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n",
      "\tat java.base/java.net.Socket.connect(Unknown Source)\n",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:136)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)\n",
      "\t... 29 more\n",
      "\n",
      "25/09/03 18:30:56 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (172.19.0.7 executor 0): org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:346)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:298)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n",
      "\tat java.base/java.net.Socket.connect(Unknown Source)\n",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:136)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)\n",
      "\t... 29 more\n",
      "\n",
      "25/09/03 18:30:56 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n",
      "25/09/03 18:30:56 WARN TaskSetManager: Lost task 0.2 in stage 1.0 (TID 4) (172.19.0.5 executor 2): TaskKilled (Stage cancelled: [SPARK_JOB_CANCELLED] Job 1 cancelled The corresponding SQL query has failed. SQLSTATE: XXKDA)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o135.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (172.19.0.6 executor 1): org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:346)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n\tat org.postgresql.Driver.connect(Driver.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:136)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:346)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n\tat org.postgresql.Driver.connect(Driver.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:136)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JJavaError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m category_counts=film_category.select(\u001B[33m\"\u001B[39m\u001B[33mcategory_id\u001B[39m\u001B[33m\"\u001B[39m).groupBy(\u001B[33m\"\u001B[39m\u001B[33mcategory_id\u001B[39m\u001B[33m\"\u001B[39m).count()\n\u001B[32m      2\u001B[39m task_1_result=category_counts.join(on=\u001B[33m\"\u001B[39m\u001B[33mcategory_id\u001B[39m\u001B[33m\"\u001B[39m, how=\u001B[33m\"\u001B[39m\u001B[33minner\u001B[39m\u001B[33m\"\u001B[39m,other=category)\\\n\u001B[32m      3\u001B[39m                              .orderBy(col(\u001B[33m\"\u001B[39m\u001B[33mcount\u001B[39m\u001B[33m\"\u001B[39m).desc()).drop(\u001B[33m\"\u001B[39m\u001B[33mlast_update\u001B[39m\u001B[33m\"\u001B[39m)\\\n\u001B[32m      4\u001B[39m                              .withColumnRenamed(\u001B[33m\"\u001B[39m\u001B[33mcount\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mnumber_of_films\u001B[39m\u001B[33m\"\u001B[39m)\\\n\u001B[32m      5\u001B[39m                              .withColumnRenamed(\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mcategory\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mtask_1_result\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/PY_internship/Spark_practice/.venv/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:285\u001B[39m, in \u001B[36mDataFrame.show\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m = \u001B[32m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_show_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/PY_internship/Spark_practice/.venv/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:303\u001B[39m, in \u001B[36mDataFrame._show_string\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    297\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[32m    298\u001B[39m         errorClass=\u001B[33m\"\u001B[39m\u001B[33mNOT_BOOL\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    299\u001B[39m         messageParameters={\u001B[33m\"\u001B[39m\u001B[33marg_name\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mvertical\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33marg_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical).\u001B[34m__name__\u001B[39m},\n\u001B[32m    300\u001B[39m     )\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[32m--> \u001B[39m\u001B[32m303\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    304\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    305\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/PY_internship/Spark_practice/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1356\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1357\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1358\u001B[39m     args_command +\\\n\u001B[32m   1359\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1361\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1362\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1363\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1365\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1366\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/PY_internship/Spark_practice/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:282\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    279\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpy4j\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mprotocol\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[32m    281\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m282\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    283\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    284\u001B[39m     converted = convert_exception(e.java_exception)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/PY_internship/Spark_practice/.venv/lib/python3.13/site-packages/py4j/protocol.py:327\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    325\u001B[39m value = OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[32m2\u001B[39m:], gateway_client)\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[32m1\u001B[39m] == REFERENCE_TYPE:\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[32m    328\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    329\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name), value)\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    332\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    333\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n",
      "\u001B[31mPy4JJavaError\u001B[39m: An error occurred while calling o135.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (172.19.0.6 executor 1): org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:346)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n\tat org.postgresql.Driver.connect(Driver.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:136)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:346)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)\n\tat org.postgresql.Driver.makeConnection(Driver.java:446)\n\tat org.postgresql.Driver.connect(Driver.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:136)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)\n\t... 29 more\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **2 Task:** Output the 10 actors whose movies rented the most, sorted in descending order.",
   "id": "92e273f11cb14f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "film_actor_data=film_actor.select(\"film_id\",\"actor_id\")\\\n",
    "                          .join(actor.select(\"actor_id\",\"first_name\",\"last_name\"),on=\"actor_id\",how=\"inner\")\\\n",
    "                          .join(film.select(\"film_id\"),on=\"film_id\",how=\"inner\")\n",
    "film_rental_rate=rental.select(\"inventory_id\")\\\n",
    "                       .join(other=inventory.select(\"inventory_id\",\"film_id\"),on=\"inventory_id\",how=\"inner\")\\\n",
    "                       .groupBy(\"film_id\").count()\n",
    "task_2_result=film_actor_data.select(\"film_id\",\"actor_id\" ,\"first_name\", \"last_name\")\\\n",
    "                             .join(other=film_rental_rate,on=\"film_id\",how=\"inner\")\\\n",
    "                             .groupBy(\"actor_id\" ,\"first_name\", \"last_name\")\\\n",
    "                             .sum(\"count\").sort(col(\"sum(count)\").desc())\\\n",
    "                             .withColumnRenamed(\"sum(count)\",\"sum\").limit(10)\n",
    "task_2_result.show()\n"
   ],
   "id": "90bd1de18b97df42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **3 Task:** Output the category of movies on which the most money was spent.",
   "id": "ee60a5df94e796a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "task_3_result=payment.select(\"amount\",\"rental_id\")\\\n",
    "                     .join(other=rental,on=\"rental_id\",how=\"inner\")\\\n",
    "                     .join(other=inventory,on=\"inventory_id\",how=\"inner\")\\\n",
    "                     .join(other=film_category,on=\"film_id\",how=\"inner\")\\\n",
    "                     .join(other=category,on=\"category_id\",how=\"inner\")\\\n",
    "                     .groupBy(\"category_id\",\"name\").sum(\"amount\").orderBy(col(\"sum(amount)\").desc())\\\n",
    "                     .select(\"name\").limit(1)\n",
    "task_3_result.show()"
   ],
   "id": "5d41af732c2ed961",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **4 Task** Output the names of movies that are not in the inventory.",
   "id": "137294489890cb09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "task_4_result=film.join(other=inventory,on=\"film_id\",how=\"left_anti\").select(\"title\")\n",
    "task_4_result.show()"
   ],
   "id": "26dcf3425129f04e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **5 Task** Output the top 3 actors who have appeared most in movies in the “Children” category. If several actors have the same number of movies, output all of them.",
   "id": "964198e06b836827"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_movies = film_category.join(other=category.filter(\"name='Children'\")\\\n",
    "                                ,on=\"category_id\",how=\"inner\").select(\"film_id\")\n",
    "film_count = film_actor.join(other=all_movies,on=\"film_id\",how=\"inner\")\\\n",
    "                       .groupBy(\"actor_id\").count()\n",
    "\n",
    "window1=Window.orderBy(col(\"count\").desc()).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "task_5_result = actor.join(other=film_count,on=\"actor_id\",how=\"left\")\\\n",
    "                     .select(\"first_name\", \"last_name\", \"count\").fillna(-1)\\\n",
    "                     .withColumn(\"rank\",dense_rank().over(window1))\\\n",
    "                     .filter(col(\"rank\")<=3)\n",
    "task_5_result.show(task_5_result.count())"
   ],
   "id": "50a9edfb04bc5233",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **6 Task** Output cities with the number of active and inactive customers (active - customer.active = 1). Sort by the number of inactive customers in descending order.",
   "id": "1f2f947a237fb09c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "task_6_result=customer.select(\"active\",\"address_id\").join(other=address,on=\"address_id\",how=\"inner\")\\\n",
    "                      .withColumn(\"inactive\", 1 - col(\"active\"))\n",
    "task_6_result=task_5_result.join(other=city,on=\"city_id\",how=\"inner\").orderBy(col(\"inactive\").desc())\\\n",
    "                           .select(\"city\",\"active\",\"inactive\").distinct()\n",
    "task_6_result.show()"
   ],
   "id": "b94d0213d6b685b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **7 Task** Output the category of movies that have the highest number of total rental hours in the cities (customer.address_id in this city), and that start with the letter “a”. Do the same for cities with a “-” symbol.",
   "id": "b4544f4d8a923876"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "film_time = rental.join(other=inventory,on=\"inventory_id\",how=\"inner\")\\\n",
    "                .join(other=film_category,on=\"film_id\",how=\"inner\")\\\n",
    "                .join(other=category,on=\"category_id\",how=\"inner\")\\\n",
    "                .withColumn(\"time\",col(\"return_date\").cast(\"long\")-col(\"rental_date\").cast(\"long\"))\\\n",
    "                .select(\"customer_id\",\"name\",\"time\").withColumnRenamed(\"name\",\"category\")\n",
    "\n",
    "\n",
    "city_category_time = film_time.join(other=customer,on=\"customer_id\",how=\"inner\")\\\n",
    "                            .join(other=address,on=\"address_id\",how=\"inner\")\\\n",
    "                            .join(other=city,on=\"city_id\",how=\"inner\")\\\n",
    "                            .groupBy(\"city\",\"category\").sum(\"time\")\\\n",
    "                            .select(\"city\",\"category\",\"sum(time)\")\n",
    "\n",
    "city_category_max_time = city_category_time.withColumn(\"city_type\",when(col(\"city\").startswith(\"A\"),\"A\")\\\n",
    "                                                       .when(col(\"city\").contains(\"-\"),\"-\")\\\n",
    "                                                       .otherwise(None))\n",
    "window2=Window.orderBy(col(\"total_time\").desc()).partitionBy(\"city_type\")\n",
    "task_7_result=city_category_max_time.groupby(\"city_type\",\"category\").sum(\"sum(time)\")\\\n",
    "                             .withColumnRenamed(\"sum(sum(time))\",\"total_time\")\\\n",
    "                             .withColumn(\"rank\",rank().over(window2))\\\n",
    "                             .filter((col(\"rank\")==1) & ((col(\"city_type\")==\"A\") | (col(\"city_type\")==\"-\")))\\\n",
    "                             .select(\"city_type\",\"category\",((col(\"total_time\")/60/60/24).cast(\"int\")).alias(\"total_time\"))\n",
    "task_7_result.show()\n",
    "\n"
   ],
   "id": "48f45093ebb2b18e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
